ami-09c813fb71547fc4f

CICD:- Continous Integration 
-----
.For continous integration we will use Jenkins

in olden days we use to follow the traditional manually deployment process:-
--------------------------------------------------------------------------
.take one server -->git clone <url> --> npm install --> downloads dependencies -->npm test --> unit test cases
                               sonar--> scan ,zip--> zip the application
.it store the application into some other artifact server. this will be the manually process every time it will take lot of time,
  Error like Human errors can occur,
  Inconsistent: Different environments or team members might execute steps differently
  Lack of Automation: Repeated efforts are required for every new build or deployment.

.so in continous integration we take one server and manage all the dependencies that is jenkins server.


*****.What is continous integration ?****
 .it is the way of integration the code from source and create artifact(file or docs) .in between source
  and creating artifact we need to install dependencies, run unit test cases scans etc .
  insted of this manually we can automate through Continus Integration process .. 
  we are using Jenkins for Continous Integration.
  
.Jenkins:- Jenkins is a plain webserver, if you install plugins then jenkins can connect 
--------   with other tools. 

Install:- take ec2 instance 
-------
sudo curl -o /etc/yum.repos.d/jenkins.repo \
    https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

sudo yum upgrade--> skips this it will lot of time.
# Add required dependencies for the jenkins package

sudo yum install fontconfig java-17-openjdk
sudo yum install jenkins
sudo systemctl start jenkins
sudo systemctl enable jenkins 

.ip-address:8080 --> to configure the jenkins
.copy the code and -->  sudo cat /var/lib/jenkins/secrets/initialAdminPassword
.take the key and past 
.create user information and save the file.

#sudo systemctl daemon-reload


(Jenkins url
( http://3.237.63.102:8080/ )

(jenkins location m-repos:-sudo cat /etc/yum.repos.d/jenkins.repo)


.jenkins will run on port 8080.

.in Jenkins every thing is called as JOB. when you trigger it is called as build.
.we have 3 stages in build
  
  .pre-build, build ,post-build.
  
.Free-style job:- manually created infra vs iaac
---------------  . you cant restore if something goes wrong
				 . we can track we did the changes
				 . cant reuse
				 . no review of chnages
				 . no version control.
				 . if you make it as pipeline and keep it in git you can reuse ,u can review, you can raise the pr.
  

.Pipeline:- is noting but Mutiple-stages creating like the output be comming input .
----------
Jenkinsfile (Declarative Pipeline)

pipeline {
    agent any 
    stages {
        stage('Build') { 
            steps {
                sh "echo this is build"
            }
        }
        stage('Test') { 
            steps {
                sh "echo this is test"
            }
        }
        stage('Deploy') { 
            steps {
                sh "echo this is Deploy"
                error 'pipeline success'
            }
        }
    }
}
.manage jenkins -->Available plugins --> pipeline stage view.

this will show the stage view .

.All the pipeline has to come from git and we need refer in jenkins

.Master Agent Architecture:- In continous integration like jenkins environment master it can't satisfy 
---------------------------  all the requirments and all the work loads so can have diffrent types of agents with 
							 with diffrent agents we can connect master vya SSH, you can provide agent ip-address
							 and passwords you can provide some labels ,while trigger the pipeline we can menction
							 which agent to run the pipeline.

.Master :- master will assign the work to agents and it will collect the log and shows.

.install for agent (sudo yum install fontconfig java-17-openjdk)

.deleteDir()--> will delete used build to same memory

.Jinkins logs:-
--------------
. /var/lib/jenkins -->Jenkins home


.Triggers:-
----------
.When ever developer push to git, it should automatically trigger pipeline
 
.we will call this as webhooks.

.when ever developers  has push to git branches we will immeditly trigger jenkins pipeline through webhook.

git hub -->go to repo-->webhook-->add-webhook-->payload url --> add jenkins url/github-webhook/
and disable SSL



-----------------------------------------------------------------------------------------------------------------------------------
Expense application integration in jenkins:-
--------------------------------------------
.take 50 gb and re-size the disk 

. install  plugins
1, stage view
2, steps utlities
3, aws credintails
4, aws steps


.http://jenkins.aws-dev-rk.online:8080/ --> To login into jenkins
.install jenkins user id in jenkins to get the password.

.what is upstrem and down stream in jenkins:-
--------------------------------------------
. when they are dependencies between multiple jobs in jenkins we can trigger pip line
  when one pip line is success.
  
 
  
  as simple the pipeline will trigger after 1 job is success.
  
 .. we are using upstrem & down stream jobs for our vpc & SG 
  
.what are the jenkins agent you are using ?
 -----------------------------------------
 .
  
  
Parallel In Sequential:-
----------------------- 
 
.we have parllel or Sequential that can destory and trigger the resources
 . vpc
 . sg
     . bastion
	 .eks
	 .acm
	 .ecr
	 .cdn
 . alb
 
 .in the stage if there is no dependencies we can go with parllel stage 
 . this way you cann trigger or destory the resources.
 
 
.vpc -->  it should create the vpc , and trigger the sg' (this is scencuatule)
-----
.SG --> it apply and it should create sg and trigger (these are parllel)
------   . bastion  
		 . rds
		 . eks
		 . acm
		 . ecr
		 . cdn
		 
		 . if every thing is created and applied
. alb

*******in projects we dont destory the infra**********************************

Destory:
-------
ECR when it destory it can trigger all  and destory
 .alb
 .cdn
 .ecr
 .acm
 .eks
 .rds
 .bastion

sequence
	 .sg
	 .vpc

. aws-command line 
 --------------
 . terraform plan -out=tfplan
 . terraform apply tfplan
 . terraform destroy -auto-approve


.STEPS:- CICD
 ------
.sg is down steam for sg

.vpc is upstrem for sg

We need to set up the eks
-----------------------------
.we need to set the aws-loadbalancer-controller
.and we need to have ingress resources (use if listerns and rules are not created) &
.use the target group binding ((use if listerns and rules are created)

.APP CICD
 =========
. after this one time setup we can create
   . Build
   . unit test
   . scan
   . build the image
   . push the image
   . heml to deploy
   
   
. 1st we need to set all the infra ready 
	.vpc,sg,bastion,rds,eks,acm,alb,ecr,cdn,

. RDS has to connect to port 3306 from bastion and from eks worker-nodes.(because pods will be in eks nodes)
. Connect to configure mysql ( mysql -h expense-dev.czn6yzxlcsiv.us-east-1.rds.amazonaws.com -u root -pExpenseApp1 )
							  . USE transactions;
							  . CREATE TABLE IF NOT EXISTS transactions (
									id INT AUTO_INCREMENT PRIMARY KEY,
									amount INT,
									description VARCHAR(255)
								);
							 . CREATE USER IF NOT EXISTS 'expense'@'%' IDENTIFIED BY 'ExpenseApp@1';
							 . GRANT ALL ON transactions.* TO 'expense'@'%';
							 . FLUSH PRIVILEGES;
							 
. Target goup banding : the docker file is very thin that they are below 1024 ports for that resion we are using 8080 in nginx conf
                        target group health check has to be 8080, and service in frontend has to be 8080 other wise we have  to run as root user.
						
. configure eks cluster --> steps.	

. then configure backend --> check the docker file ---> jenkins file --> push the image to ECR
   . Use ECR image credintails to login and push the image to docker hub.	
   
   . create the backend expense in jenkins
   . add helm bakend file to jenkins backend make user 
   
   . use sed editor to insert that will be tempory edit (if you want use as perminent editor (sed -i '1 hello') to append (sed '1 a hi') (sed 's/local/LOCAL')
   
   . after backend is configure  check --> kubens expense --> helm list --> kubectl get pods --> kubectl logs 
   
   
   . Same way configure the frontend .
     . docker file --> jenkins-file--push the image
	 
	 . after configure the tgb 
	 (. check -->  kubectl get target group binding )
	 
	 
. Target goup banding :- you can use target group you can use any one of this either ingress or target group .
                         (if you use ingress  remove listerns,rules,target group from 70-alb because
						  we have created our own resources because generally we dont give access to k8 for external useage 
						  what ever we had in our hands we can create them using terraform with the kubernetes we can manage them dynamic )
						  
						  
						  
-------------------------------------------------------------------------------------------------------------------------------------------
DevSecOps:- 
---------
.sonar.public.aws-dev-rk.online:9000 (or) (ip-address:9000)


. shift-left:- . shifting the security scannings and testing in dev before pushing the code to main branch
------------   . when developer push the code to feature branches we should scan and test
			   
			   1, static source code analysis:->  we can use SonarQube (wether you code is execute properly or not)
			   
			   2, static application security testing:-> we can use SonarQube (means in you code do u have any security issue are there)
			   
			   3, dynamic application security testing :-> Veracode (if you application is in public how public attach on it)
			   
			   4, dependencies scan:-> NexusIQ/ Git hub, blackduck
			   
			   5, image scanning :-> ECR Scanning
			   
. unit testing :-> it has to be done by developers
. functional testing --> it has to be done by testers (multiple-function if testing will be functional testing)
			   
			   
.SonarQube: if you want to configure the sonar scan you do it by docker image or by ec2 instance(t3.medium) and ami id
----------
            .steps-->
			 . install the plugins of sonarqube
. Multi branch pipeline:- . A pipeline should be there for every feature branch to support their development
------------------------
			 . 
			 
.Sonar quality gate:- we have quality gate for our project implimented on  overall code & new code  we have set the quality gate
-------------------
. 0 issues
. 0 bugs
. security rating A
. Maintainability rating A 
. code coverage 80%
. code smells 0
. vulnerabilities 0


.Shared pipeline Continuous Integration (CI) and
                 Continuous Deployment (CD)		:- pipeline is a function it takes input and runs the pipeline
--------------------------------------------	   . any number of project can call this shared pipeline at a time
												     no need to maintain pipeline differently for diffrent project
													 easy to update & enforce standed at the high level.
													 
. exmpl: backend file owner is developer & jenkins file owner is devops team 
        
		.In a one repositry two team cant work it will bring big problm
		. if we have a chnage we cant distrub the development & devops we can keep 
		
		ci:-
		. image is our output
		. dev --> use the image, but diffrent config values-dev.yaml ecr.
		. qa/uat/prod --> use the same image but diffrent config values-prod.yaml
		
Build once run any where:-
------------------------
*.we follow seperate CI and CD ,Because we can use CD job to deploy our application to multiple environment
 build once and run any where

. (when ever the developer push the code no need to deploy worst case 50th time deploy shouid be done.)

. our pipeline will have an option to choose deploy or not
. backend --> upstrem
. backend-deploy --> just helm charts.
		  
. Central devops engineers:- They will look into 
---------------------------
. Ansible Roles
. Terraform Modules
. jenkins central pipelines

1. what is the programming language
2. what is the deployment platform
3. what is the build tool --> in java tool maven and gradle, ant


.Project Onboarding:-
--------------------
1, what is your programming Language
2, "    "   "   deployment platform
3, "    "   "   branching strategy (yours is feature branching strategy)

.we need create a folder in jenkins SOP:- If a new project comes, we meet development team and setup below things
---------------------------------------
. Jenkis folder
. sonarqube
. k8 name space
. ecr repos
. veracode we need to create the target
. github dependabot
. dockerfile
. helm charts